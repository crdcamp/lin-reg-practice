{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43520863",
   "metadata": {},
   "source": [
    "# Imputation of missing values\n",
    "\n",
    "After a bit of investigating, it seems like we have a pretty big rabbit hole to go down here. There's a bunch of different ways to handle this missing data and I want to test at least 2 of them to compare the results. We'll start with [Sklearn's](https://scikit-learn.org/stable/modules/impute.html) options, then check out [LightGBM](https://lightgbm.readthedocs.io/en/stable/). Depending on where that takes us, I'll determine if exploring other options is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d318bea",
   "metadata": {},
   "source": [
    "# Sklearn Native Imputation Methods\n",
    "\n",
    "For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data. See the glossary entry on imputation.\n",
    "\n",
    "## Univariate vs. Multivariate Imputation\n",
    "\n",
    "One type of imputation algorithm is univariate, which imputes values in the i-th feature dimension using only non-missing values in that feature dimension (e.g. `SimpleImputer`). By contrast, multivariate imputation algorithms use the entire set of available feature dimensions to estimate the missing values (e.g. `IterativeImputer`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fdff81",
   "metadata": {},
   "source": [
    "## Sklearn Univariate feature imputation\n",
    "\n",
    "The `SimpleImputer` class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.\n",
    "\n",
    "The following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean value of the columns (axis 0) that contain the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d31ba84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.         2.        ]\n",
      " [6.         3.66666667]\n",
      " [7.         6.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(imp.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152df589",
   "metadata": {},
   "source": [
    "The `SimpleImputer` class also supports sparse matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "def8b582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 2.]\n",
      " [6. 3.]\n",
      " [7. 6.]]\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "X = sp.csc_matrix([[1, 2], [0, -1], [8, 4]])\n",
    "imp = SimpleImputer(missing_values=-1, strategy='mean')\n",
    "imp.fit(X)\n",
    "X_test = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]])\n",
    "print(imp.transform(X_test).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a8174",
   "metadata": {},
   "source": [
    "Note that this format is not meant to be used to implicitly store missing values in the matrix because it would densify it at transform time. Missing values encoded by 0 must be used with dense input.\n",
    "\n",
    "The `SimpleImputer` class also supports categorical data represented as string values or pandas categoricals when using the 'most_frequent' or 'constant' strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12ba1261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a' 'x']\n",
      " ['a' 'y']\n",
      " ['a' 'y']\n",
      " ['b' 'y']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([[\"a\", \"x\"],\n",
    "                   [np.nan, \"y\"],\n",
    "                   [\"a\", np.nan],\n",
    "                   [\"b\", \"y\"]], dtype=\"category\")\n",
    "imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "print(imp.fit_transform(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414d94c5",
   "metadata": {},
   "source": [
    "## Sklearn Multivariate Feature Imputation\n",
    "\n",
    "A more sophisticated approach is to use the `IterativeImputer` class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output `y` and the other feature columns are treated as inputs `X`. A regressor is fit on `(X, y)` for known `y`. Then, the regressor is used to predict the missing values of `y`. This is done for each feature in an iterative fashion, and then is repeated for `max_iter` imputation rounds. The results of the final imputation round are returned.\n",
    "\n",
    "**Note**\n",
    "\n",
    "This estimator is still experimental for now: default parameters or details of behaviour might change without any deprecation cycle. Resolving the following issues would help stabilize `IterativeImputer`: convergence criteria (#14338) and default estimators (#13286). To use it, you need to explicitly import `enable_iterative_imputer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fe0800f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.]\n",
      " [ 6. 12.]\n",
      " [ 3.  6.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\n",
    "X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\n",
    "# the model learns that the second feature is double the first\n",
    "print(np.round(imp.transform(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54bcc87",
   "metadata": {},
   "source": [
    "Both `SimpleImputer` and `IterativeImputer` can be used in a Pipeline as a way to build a composite estiamtor that supports imputation. We'll explore that here.\n",
    "\n",
    "## Example: Imputing Missing Values Before Building an Estimator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lin-reg-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
